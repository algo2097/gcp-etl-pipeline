{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d55b21b4-4c6e-4861-b203-74e9335052d6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Importing necessry libraries and creating a spark session.\n",
    "from pyspark.sql import SparkSession, functions as func, types as t, Window\n",
    "import datetime as dt\n",
    "import sys\n",
    "#from google.cloud.dataproc_spark_connect import DataprocSparkSession\n",
    "#from google.cloud.dataproc_v1 import Session\n",
    "\n",
    "project  = \"enrichment-etl-jlr\"\n",
    "location = \"europe-west2\"\n",
    "\n",
    "#session_template = \"runtime-fe28\"\n",
    "#session = Session()\n",
    "#session.session_template = f\"projects/{project}/locations/{location}/sessionTemplates/{session_template}\"\n",
    "\n",
    "spark = (SparkSession.builder\n",
    "         .appName(\"step4-eda_raw_tables\")\n",
    "         .getOrCreate())\n",
    "\n",
    "# Setting up some configuration variables.\n",
    "PROJECT_ID = 'enrichment-etl-jlr'\n",
    "BUCKET = 'enrichment-etl-jlr'\n",
    "RUN_DATE = dt.date.today().strftime(\"%Y%m%d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a6b86120-a4fd-4f97-99e5-d275cc79f889",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Loading the BigQuery tables created\n",
    "base_df = spark.read.format(\"bigquery\").option(\"table\", \"enrichment-etl-jlr.raw.base_data\").load()\n",
    "options_df = spark.read.format(\"bigquery\").option(\"table\", \"enrichment-etl-jlr.raw.options_data\").load()\n",
    "vehicle_df  = spark.read.format(\"bigquery\").option(\"table\", \"enrichment-etl-jlr.raw.vehicle_line_mapping\").load()\n",
    "\n",
    "# Output Directory\n",
    "OUT_BASE = f\"gs://{BUCKET}/transformed/clean/base_data/\"\n",
    "OUT_OPTS = f\"gs://{BUCKET}/transformed/clean/options_data/\"\n",
    "OUT_VLM = f\"gs://{BUCKET}/transformed/clean/vehicle_line_mapping/\"\n",
    "OUT_BASE_PARQUET = f\"gs://{BUCKET}/transformed/clean/base_data_parquet/\"\n",
    "OUT_OPTS_PARQUET = f\"gs://{BUCKET}/transformed/clean/options_data_parquet/\"\n",
    "OUT_VLM_PARQUET = f\"gs://{BUCKET}/transformed/clean/vehicle_line_mapping_parquet/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a03e15cb-0b32-4d11-bc56-63258ca21bde",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'\\nprint(f\"Initial row count (base_df): {base_df.count()}\")\\ntrue_duplicates_check = base_df.groupBy(*base_df.columns).count().filter(func.col(\"count\") > 1)\\nprint(f\"Number of true duplicate groups found (base_df): {true_duplicates_check.count()}\")\\n\\nprint(f\"Initial row count (options_df): {options_df.count()}\")\\ntrue_duplicates_check = options_df.groupBy(*options_df.columns).count().filter(func.col(\"count\") > 1)\\nprint(f\"Number of true duplicate groups found (options_df): {true_duplicates_check.count()}\")\\n\\nprint(f\"Initial row count (vehicle_df): {vehicle_df.count()}\")\\ntrue_duplicates_check = vehicle_df.groupBy(*vehicle_df.columns).count().filter(func.col(\"count\") > 1)\\nprint(f\"Number of true duplicate groups found (vehicle_df): {true_duplicates_check.count()}\")\\n'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# True Duplicates in the Dataset\n",
    "\n",
    "\"\"\"\n",
    "print(f\"Initial row count (base_df): {base_df.count()}\")\n",
    "true_duplicates_check = base_df.groupBy(*base_df.columns).count().filter(func.col(\"count\") > 1)\n",
    "print(f\"Number of true duplicate groups found (base_df): {true_duplicates_check.count()}\")\n",
    "\n",
    "print(f\"Initial row count (options_df): {options_df.count()}\")\n",
    "true_duplicates_check = options_df.groupBy(*options_df.columns).count().filter(func.col(\"count\") > 1)\n",
    "print(f\"Number of true duplicate groups found (options_df): {true_duplicates_check.count()}\")\n",
    "\n",
    "print(f\"Initial row count (vehicle_df): {vehicle_df.count()}\")\n",
    "true_duplicates_check = vehicle_df.groupBy(*vehicle_df.columns).count().filter(func.col(\"count\") > 1)\n",
    "print(f\"Number of true duplicate groups found (vehicle_df): {true_duplicates_check.count()}\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "81cb8dc6-6c86-45f3-a03a-ac7cba4ce4a9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Duplicates Dropped\n",
    "base_df = base_df.dropDuplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ed6801df-d081-4937-8be9-f2e154314f73",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Replacing all the null values with 'Unknown'\n",
    "base_df = base_df.fillna({\"vin\": \"Unknown\", \"option_desc\": \"Unknown\", \"model_text\": \"Unknown\"})\n",
    "options_df = options_df.fillna({\"option_desc\": \"Unknown\"})\n",
    "vehicle_df = vehicle_df.fillna({\"platform\": \"Unknown\", \"nameplate_display\": \"Unknown\"})\n",
    "\n",
    "# Doing some basic cleaning and deriving moedel and model_name columns\n",
    "base_df = (\n",
    "    base_df\n",
    "    .withColumn(\"model\", func.expr(\"substring(model_text, 1, 4)\"))\n",
    "    .withColumn(\"model\", func.when(func.col(\"model\").isin(\"Free\", \"    \"), \"Unknown\").otherwise(func.col(\"model\")))\n",
    "    .withColumn(\"model_name\", func.expr(\"substring(model_text, 6, greatest(length(model_text)-5, 0))\"))\n",
    "    .withColumn(\"model_name\",\n",
    "                func.when((func.col(\"model_name\") == \"\") | (func.col(\"model_name\") == \"ander /LR2\"), \"Unknown\")\n",
    "                  .otherwise(func.col(\"model_name\")))\n",
    ")\n",
    "\n",
    "# Deriving net_quantity and net_sales_price columns\n",
    "agg_base_df = (\n",
    "    base_df.groupBy('vin', 'option_code', 'option_desc', 'model_name', 'model').agg(\n",
    "        func.sum('option_quantities').alias('net_quantity'),\n",
    "        func.sum('sales_price').alias('net_sales_price')\n",
    "        ).withColumn(\n",
    "            'net_quantity',\n",
    "            func.col('net_quantity').cast(t.DoubleType())\n",
    "            ).withColumn(\n",
    "                'net_sales_price',\n",
    "                func.col('net_sales_price').cast(t.DoubleType())\n",
    "                )\n",
    "    )\n",
    "\n",
    "#options_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "190d2d7e-6cc3-4586-9510-9a03b454f7dc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folder is clean or does not exist: gs://enrichment-etl-jlr/transformed/clean/base_data/\nFolder is clean or does not exist: gs://enrichment-etl-jlr/transformed/clean/options_data/\nFolder is clean or does not exist: gs://enrichment-etl-jlr/transformed/clean/vehicle_line_mapping/\nFolder is clean or does not exist: gs://enrichment-etl-jlr/transformed/clean/base_data_parquet/\nFolder is clean or does not exist: gs://enrichment-etl-jlr/transformed/clean/options_data_parquet/\nFolder is clean or does not exist: gs://enrichment-etl-jlr/transformed/clean/vehicle_line_mapping_parquet/\n"
     ]
    }
   ],
   "source": [
    "# Checking and emptying the parquet folders.\n",
    "def folder_exists(path: str) -> bool:\n",
    "    try:\n",
    "        return len(dbutils.fs.ls(path)) > 0\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "parquet_folder_paths = [OUT_BASE, OUT_OPTS, OUT_VLM, OUT_BASE_PARQUET, OUT_OPTS_PARQUET, OUT_VLM_PARQUET]\n",
    "for parquet_folder_path in parquet_folder_paths:\n",
    "    if folder_exists(parquet_folder_path):\n",
    "        print(f\"Folder exists with data: {parquet_folder_path}\")\n",
    "        dbutils.fs.rm(parquet_folder_path, recurse=True)\n",
    "        print(\"Deleted old data\")\n",
    "    else:\n",
    "        print(f\"Folder is clean or does not exist: {parquet_folder_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "23e7b32a-9df8-4d3f-8eb0-1d383354daf8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Loading the data to parquet files\n",
    "(\n",
    "    agg_base_df\n",
    "    .withColumn(\"run_date\", func.lit(RUN_DATE))\n",
    "    .write.mode(\"overwrite\").partitionBy(\"run_date\")\n",
    "    .parquet(OUT_BASE)\n",
    ")\n",
    "\n",
    "(\n",
    "    options_df\n",
    "    .withColumn(\"run_date\", func.lit(RUN_DATE))\n",
    "    .write.mode(\"overwrite\").partitionBy(\"run_date\")\n",
    "    .parquet(OUT_OPTS)\n",
    ")\n",
    "\n",
    "(\n",
    "    vehicle_df\n",
    "    .withColumn(\"run_date\", func.lit(RUN_DATE))\n",
    "    .write.mode(\"overwrite\").partitionBy(\"run_date\")\n",
    "    .parquet(OUT_VLM)\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "68ff5399-f9e1-4872-97e0-e7e85efc1084",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "1725394"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agg_base_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2862fee0-d40b-4ee3-9d18-509d4dc05299",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "52a7d2cc-9385-482a-afe3-75f105f59991",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "EDA_Raw_Tables",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}